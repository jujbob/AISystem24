{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["🤗 라이브러리 설치후 런타임 재시작 🤗"],"metadata":{"id":"Ys8026gZQX-x"}},{"cell_type":"code","source":["!pip install transformers sentencepiece datasets accelerate -qqq"],"metadata":{"id":"SEPyGKhQ3jas"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.1.3. 🤗 허브에서 불러오기"],"metadata":{"id":"_TFgeT78cTBA"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","# dataset = load_dataset('wikitext', 'wikitext-103-v1')\n"],"metadata":{"id":"TWC41JcTeq44"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.2. 데이터 전처리 : Dataset.map()"],"metadata":{"id":"bJsRnCeR0z3g"}},{"cell_type":"markdown","source":["### 3.2.1. Dataset.map() 기본 용법"],"metadata":{"id":"oA7RTBoaetF_"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset = load_dataset('imdb', split=\"train[:1000]\")\n","\n","\n"],"metadata":{"id":"XBwP3gRP03B6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset[0]['length']"],"metadata":{"id":"q9LxHJ83MHDM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(train_dataset[0]['text'])"],"metadata":{"id":"Evlta8R7MK2c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.2.2. 토크나이저 적용"],"metadata":{"id":"mIKnmnkGft3Z"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","\n"],"metadata":{"id":"J1pmdsG5geRP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.2.3. 병렬 처리"],"metadata":{"id":"G4DJORnXg47K"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","\n","def tokenize(examples):\n","    return tokenizer(examples[\"text\"], padding=True, max_length=50)\n","\n","tokenized_dataset = dataset.map(tokenize, batched=True)"],"metadata":{"id":"fsK0iR4zg7gs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. 훈련 API (Trainer)"],"metadata":{"id":"3VqLvsLEhpFD"}},{"cell_type":"markdown","source":["## 4.2. Trainer API 사용 예제"],"metadata":{"id":"14BJlvDNhwwB"}},{"cell_type":"code","source":["from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\n","from transformers import Trainer, TrainingArguments\n","from datasets import load_dataset\n","\n","\n","# 1. 작업 정의: 문장 생성\n","\n","# 2. 학습 데이터 로딩\n","train_dataset = load_dataset('wikitext', 'wikitext-103-raw-v1', split='train[:1000]')\n","\n","\n","# 3. 토크나이저와 모델 로딩\n","\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","\n","\n","# 4. 학습 데이터 전처리\n","def tokenize_function(examples):\n","\toutput = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n","\treturn output\n","\n","# map 함수를 이용한 토크나이징\n","tokenized_datasets = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n","\n","# 데이터 콜레이터 설정\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","\n"],"metadata":{"id":"czKts-pBnlq8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 5. 모델 학습\n","# 학습을 위한 설정\n","training_args = TrainingArguments(\n","\toutput_dir=\"./gpt2_finetuned\",\n","\toverwrite_output_dir=True,\n","\tnum_train_epochs=3,\n","\tper_device_train_batch_size=2,\n","\tsave_steps=1000,\n","\tsave_total_limit=2,\n",")\n","\n","# Trainer 객체를 생성\n","trainer = Trainer(\n","\tmodel=model,\n","\targs=training_args,\n","\tdata_collator=data_collator,\n","\ttrain_dataset=tokenized_datasets,\n",")\n","\n","# 학습 시작\n","trainer.train()"],"metadata":{"id":"_sb0Nlz-oKWX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.3. DataCollator"],"metadata":{"id":"d1_qDzW5hzxn"}},{"cell_type":"markdown","source":["### DataCollatorForLanguageModeling"],"metadata":{"id":"cTusCWaDo5aL"}},{"cell_type":"code","source":["from transformers import DataCollatorForLanguageModeling\n","\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer, mlm=False\n",")"],"metadata":{"id":"5RFs7yJGo1lN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### DataCollatorForTokenClassification"],"metadata":{"id":"QaE2L-u2o9Q4"}},{"cell_type":"code","source":["from transformers import DataCollatorForTokenClassification\n","\n","data_collator = DataCollatorForTokenClassification(\n","    tokenizer=tokenizer\n",")"],"metadata":{"id":"ldNZFv3Fo4Y0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### DataCollatorWithPadding"],"metadata":{"id":"_aC3IIhJpAJB"}},{"cell_type":"code","source":["from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(\n","    tokenizer=tokenizer\n",")"],"metadata":{"id":"1u18uVwho_kg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### DataCollatorForSeq2Seq"],"metadata":{"id":"kO07gC9GpCXy"}},{"cell_type":"code","source":["from transformers import DataCollatorForSeq2Seq\n","\n","data_collator = DataCollatorForSeq2Seq(\n","    tokenizer=tokenizer\n",")"],"metadata":{"id":"d8OimlBTpB1U"},"execution_count":null,"outputs":[]}]}