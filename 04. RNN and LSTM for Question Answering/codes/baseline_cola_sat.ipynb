{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"WEiWqpvJzG22"},"outputs":[],"source":["import dill\n","import time\n","import random\n","import numpy as np\n","from sklearn.metrics import roc_curve, auc\n","\n","import nltk\n","\n","nltk.download(\"punkt\")\n","from nltk.tokenize import word_tokenize\n","\n","import torch\n","import torch.nn as nn\n","\n","from torchtext.data import Field\n","from torchtext.data import TabularDataset\n","from torchtext.data import BucketIterator\n","from torchtext.data import Iterator\n","from torchtext.data import Dataset, Example"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DXvvT31XzRa4"},"outputs":[],"source":["RANDOM_SEED = 2020\n","torch.manual_seed(RANDOM_SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","np.random.seed(RANDOM_SEED)\n","random.seed(RANDOM_SEED)\n","\n","# DATA_PATH = \"data/processed/\""]},{"cell_type":"markdown","metadata":{"id":"EvjX2S_WkRF8"},"source":["## 데이터 불러오기\n","- `torchtext.Field`를 이용해 각각의 필드를 정의해줍니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_c6fS1QNybyg"},"outputs":[],"source":["TEXT = Field(\n","    sequential=True,\n","    use_vocab=True,\n","    tokenize=word_tokenize,\n","    lower=True,\n","    batch_first=True,\n",")\n","LABEL = Field(\n","    sequential=False,\n","    use_vocab=False,\n","    batch_first=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"s8toac_xypfU"},"source":["수능 데이터를 불러오는 코드입니다. `DATA_PATH`가 데이터가 있는 폴더로 정확히 입력되어 있는지 확인해주세요.\n","\n","`+ cola 데이터 불러오는 코드`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k8Twwhw9zPdJ"},"outputs":[],"source":["sat_train_data, sat_valid_data, sat_test_data = TabularDataset.splits(\n","    path=DATA_PATH,\n","    train=\"sat_train.tsv\",\n","    validation=\"sat_valid.tsv\",\n","    test=\"sat_test.tsv\",\n","    format=\"tsv\",\n","    fields=[(\"text\", TEXT), (\"label\", LABEL)],\n","    skip_header=1,\n",")\n","\n","sat_train_iterator, sat_valid_iterator, sat_test_iterator = BucketIterator.splits(\n","    (sat_train_data, sat_valid_data, sat_test_data),\n","    batch_size=8,\n","    device=None,\n","    sort=False,\n",")\n","###\n","###\n","###\n","cola_train_data, cola_valid_data, cola_test_data = TabularDataset.splits(\n","    path=DATA_PATH,\n","    train=\"cola_train.tsv\",\n","    validation=\"cola_valid.tsv\",\n","    test=\"cola_test.tsv\",\n","    format=\"tsv\",\n","    fields=[(\"text\", TEXT), (\"label\", LABEL)],\n","    skip_header=1,\n",")\n","\n","cola_train_iterator,cola_valid_iterator, cola_test_iterator = BucketIterator.splits(\n","    (cola_train_data, cola_valid_data, cola_test_data),\n","    batch_size=8,\n","    device=None,\n","    sort=False,\n",")\n","\n","TEXT.build_vocab(sat_train_data, min_freq=2)"]},{"cell_type":"markdown","metadata":{"id":"NaBRroeFkRF8"},"source":["## LSTM Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ycMzmLHzbI9"},"outputs":[],"source":["class LSTMClassifier(nn.Module):\n","    def __init__(self, num_embeddings, embedding_dim, hidden_size, num_layers, pad_idx):\n","        super().__init__()\n","        self.embed_layer = nn.Embedding(\n","            num_embeddings=num_embeddings,\n","            embedding_dim=embedding_dim,\n","            padding_idx=pad_idx\n","        )\n","        self.lstm_layer = nn.LSTM(\n","            input_size=embedding_dim,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            bidirectional=True,\n","            dropout=0.5\n","        )\n","        self.last_layer = nn.Sequential(\n","            nn.Linear(hidden_size * 2, hidden_size),\n","            nn.Dropout(0.5),\n","            nn.LeakyReLU(),\n","            nn.Linear(hidden_size, 1),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, x):\n","        embed_x = self.embed_layer(x)\n","        output, (_, _) = self.lstm_layer(embed_x)\n","        last_output = output[:, -1, :]\n","        last_output = self.last_layer(last_output)\n","        return last_output"]},{"cell_type":"markdown","metadata":{"id":"Jmn8dpXCy9Ik"},"source":["Train, Evaluate, Test 를 정의합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xHX9pXijzdKM"},"outputs":[],"source":["def train(model: nn.Module, iterator: Iterator, optimizer: torch.optim.Optimizer, criterion: nn.Module, device: str):\n","    model.train()\n","    epoch_loss = 0\n","\n","    for _, batch in enumerate(iterator):\n","        optimizer.zero_grad()\n","\n","        text = batch.text\n","        if text.shape[0] > 1:\n","            label = batch.label.type(torch.FloatTensor)\n","            text = text.to(device)\n","            label = label.to(device)\n","            output = model(text).flatten()\n","            loss = criterion(output, label)\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)\n","\n","\n","def evaluate(model: nn.Module, iterator: Iterator, criterion: nn.Module, device: str):\n","    model.eval()\n","    epoch_loss = 0\n","\n","    with torch.no_grad():\n","        for _, batch in enumerate(iterator):\n","            text = batch.text\n","            label = batch.label.type(torch.FloatTensor)\n","            text = text.to(device)\n","            label = label.to(device)\n","            output = model(text).flatten()\n","            loss = criterion(output, label)\n","            epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)\n","\n","\n","def test(model: nn.Module, iterator: Iterator, device: str):\n","    model.eval()\n","    with torch.no_grad():\n","        y_real = []\n","        y_pred = []\n","        for batch in iterator:\n","            text = batch.text\n","            label = batch.label.type(torch.FloatTensor)\n","            text = text.to(device)\n","            output = model(text).flatten().cpu()\n","            y_real += [label]\n","            y_pred += [output]\n","        y_real = torch.cat(y_real)\n","        y_pred = torch.cat(y_pred)\n","\n","    fpr, tpr, _ = roc_curve(y_real, y_pred)\n","    auroc = auc(fpr, tpr)\n","\n","    return auroc\n","\n","\n","def epoch_time(start_time: int, end_time: int):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"]},{"cell_type":"markdown","metadata":{"id":"nqYq9AypkRF9"},"source":["## 모델 학습하기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"so3L6wyQzeEI"},"outputs":[],"source":["PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","N_EPOCHS = 20\n","\n","lstm_classifier = LSTMClassifier(\n","    num_embeddings=len(TEXT.vocab),\n","    embedding_dim=100,\n","    hidden_size=200,\n","    num_layers=4,\n","    pad_idx=PAD_IDX,\n",")\n","if torch.cuda.is_available():\n","    device = \"cuda:0\"\n","else:\n","    device = \"cpu\"\n","_ = lstm_classifier.to(device)\n","\n","optimizer = torch.optim.Adam(lstm_classifier.parameters())\n","bce_loss_fn = nn.BCELoss()\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","\n","    train_loss = train(lstm_classifier, sat_train_iterator, optimizer, bce_loss_fn, device)\n","    valid_loss = evaluate(lstm_classifier, sat_valid_iterator, bce_loss_fn, device)\n","\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n","    print(f\"\\tTrain Loss: {train_loss:.5f}\")\n","    print(f\"\\t Val. Loss: {valid_loss:.5f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"am8qb8SNzS4u"},"outputs":[],"source":["_ = lstm_classifier.cpu()\n","test_auroc = test(lstm_classifier, sat_test_iterator, \"cpu\")\n","\n","print(f\"SAT Dataset Test AUROC: {test_auroc:.5f}\")"]},{"cell_type":"markdown","metadata":{"id":"_gi2MGyQrsKF"},"source":["# cola 데이터와 sat 데이터를 사용해서 학습"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oG10JRxmr0nL"},"outputs":[],"source":["lstm_classifier = LSTMClassifier(\n","    num_embeddings=len(TEXT.vocab),\n","    embedding_dim=100,\n","    hidden_size=200,\n","    num_layers=4,\n","    pad_idx=PAD_IDX,\n",")\n","if torch.cuda.is_available():\n","    device = \"cuda:0\"\n","else:\n","    device = \"cpu\"\n","_ = lstm_classifier.to(device)\n","\n","optimizer = torch.optim.Adam(lstm_classifier.parameters())\n","bce_loss_fn = nn.BCELoss()\n","\n","for epoch in range(5):\n","\n","    start_time = time.time()\n","\n","    train_loss = train(lstm_classifier, cola_train_iterator, optimizer, bce_loss_fn, device)\n","    valid_loss = evaluate(lstm_classifier, cola_valid_iterator, bce_loss_fn, device)\n","\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n","    print(f\"\\tTrain Loss: {train_loss:.5f}\")\n","    print(f\"\\t Val. Loss: {valid_loss:.5f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tPmHjfjhKuCM"},"outputs":[],"source":["if torch.cuda.is_available():\n","    device = \"cuda:0\"\n","else:\n","    device = \"cpu\"\n","_ = lstm_classifier.to(device)\n","\n","optimizer = torch.optim.Adam(lstm_classifier.parameters())\n","bce_loss_fn = nn.BCELoss()\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","\n","    train_loss = train(lstm_classifier, sat_train_iterator, optimizer, bce_loss_fn, device)\n","    valid_loss = evaluate(lstm_classifier, sat_valid_iterator, bce_loss_fn, device)\n","\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n","    print(f\"\\tTrain Loss: {train_loss:.5f}\")\n","    print(f\"\\t Val. Loss: {valid_loss:.5f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"psfOPOF1sEmU"},"outputs":[],"source":["_ = lstm_classifier.cpu()\n","test_auroc = test(lstm_classifier, sat_test_iterator, \"cpu\")\n","\n","print(f\"SAT Dataset Test AUROC: {test_auroc:.5f}\")"]},{"cell_type":"markdown","metadata":{"id":"wpiV9rcQIt2F"},"source":["# cola 데이터로 pretrain을 하고 sat 데이터로 학습을 했는데도 점수가 떨어진다면 그 이유는?"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"}},"nbformat":4,"nbformat_minor":0}
